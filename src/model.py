import torch
import torch.nn as nn
import math
from tqdm import tqdm

class transitionDataset(torch.utils.data.Dataset):
    def __init__(self, M_g, M_c, if_train):
        """
        Args:
            M_g : a concept ordering transition matrix derived from the concept ordering of the given sentences in the Commongen dataset
            M_c : a concept ordering transition matrix derived from the number of paths between concepts in the conceptnet
            if_train (boolean): if is in training or validation
        """        
        self.M_g = M_g
        self.M_c = M_c
        self.dataset = []

        if if_train:
            for i in tqdm(range(self.M_g.shape[0])):
                for j in range(self.M_g.shape[1]):
                    if self.M_c[i][j] != 0:
                        self.dataset.append((i, j))
        else:
             for i in tqdm(range(self.M_g.shape[0])):
                for j in range(self.M_g.shape[1]):
                    if self.M_g[i][j] != 0:
                        self.dataset.append((i, j))
        
        print("Dataset initilzed")
    

    def __len__(self):
        return len(self.dataset)


    def __getitem__(self, index):
        (i, j) = self.dataset[index]
        #Here we return items are 
        return i, j, torch.tensor(self.M_g[i][j], dtype=torch.float), torch.tensor(self.M_c[i][j], dtype=torch.float)


class transitionModel(nn.Module):
    def __init__(self, vocab_size, embed_size, pretrained_emb):
        super().__init__()
        self.vocab_size = vocab_size
        self.embed_size = embed_size
        self.pretrained_emb = pretrained_emb

        # v, w embedding
        self.v = nn.Embedding.from_pretrained(pretrained_emb,freeze=False)
        self.w = nn.Embedding.from_pretrained(pretrained_emb,freeze=False)

        #bias
        self.bias_v = nn.Embedding(vocab_size,1)
        self.bias_w = nn.Embedding(vocab_size,1)

        #Random initilize the biases
        initrange = 0.5 / self.embed_size
        self.bias_v.weight.data.uniform_(-initrange, initrange)
        self.bias_w.weight.data.uniform_(-initrange, initrange)
    

    def forward(self, i, j):
        #embedding
        vi = self.v(i)
        wj = self.w(j)
        #bias
        bi = self.bias_v(i).reshape(-1,)
        bj = self.bias_w(j).reshape(-1,)
        #Calculate the output
        o = torch.mul(vi,wj) / math.sqrt(self.embed_size)
        o = torch.sum(o, dim=1)
        o = o + bi + bj
        o = torch.sigmoid(o)
        return o
    
    
    def loss_func(self, output, Mg_p, Mc_p):
        """The loss function in the training
         
        Args:
            output : o is the output generated by forward function
            Mg_p : The transition probability from concept i to concept j in the Mg
            Mc_p : he transition probability from concept i to concept j in the Mc

        Returns:
            _type_: Mean squared loss as shown in Eq 7
        """        
        alpha = 0.3
        loss = 0
        for i in range(len(output)):    
            #Because we have assured that M_c[i]
            l_c =  torch.pow(output[i]- Mc_p[i], 2)
            
            if Mg_p[i]> 0:
                l_p = torch.pow(output[i]- Mg_p[i], 2 )
                loss = loss + (1-alpha) * l_p + alpha * l_c
            else:
                loss = loss + l_c

        return loss / len(output)
    
    
    def valid_loss(self, output, Mg_p):
        """The loss function in validation, we here only use the probability from M_g as in shown in Eq 10

        Args:
            output : o is the output generated by forward function
            Mg_p : The transition probability from concept i to concept j in the Mg

        Returns:
            _type_: Mean squared loss as shown in Eq 10, we return the sum because we would devide in the main function
        """        
        loss = torch.pow(output - Mg_p, 2)
        return loss.sum()
     

    
    def get_matrix(self):
        return self.v.weight.data.cpu(), self.w.weight.data.cpu(), self.bias_v.weight.data.cpu(), self.bias_w.weight.data.cpu()


    


