import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from tqdm import tqdm
import os


class transitionDataset(torch.utils.data.Dataset):
    def __init__(self, M_g, M_c):
        """
        Args:
            M_g : a concept ordering transition matrix derived from the concept ordering of the given sentences in the Commongen dataset
            M_c : a concept ordering transition matrix derived from the number of paths between concepts in the conceptnet
            M_p : which transtion between concepts would be considered
            if_train (boolean): if is in training or validation
        """        
        self.M_g = M_g
        self.M_c = M_c
        self.dataset = []

        for i in range(self.M_g.shape[0]):
            self.dataset.append(i)
    

    def __len__(self):
        return len(self.dataset)


    def __getitem__(self, index):
        i = self.dataset[index]
        #Here we return items are 
        return i, torch.tensor(self.M_g[i], dtype=torch.float), torch.tensor(self.M_c[i], dtype=torch.float)

        

class transitionModel(nn.Module):
    def __init__(self, vocab_size, embed_size, pretrained_emb):
        super(transitionModel,self).__init__()
        self.vocab_size = vocab_size
        self.embed_size = embed_size
        self.pretrained_emb = pretrained_emb
        self.h_d = 300
        
        # v, w embedding
        self.v = nn.Embedding.from_pretrained(pretrained_emb,freeze=False)
        self.w = nn.Embedding.from_pretrained(pretrained_emb,freeze=False)


        self.hidden1 = nn.Linear(self.embed_size, self.h_d)
        self.hidden2 = nn.Linear(self.h_d, self.h_d)
        self.cos = nn.CosineSimilarity(dim=1, eps=1e-6) 


    

    def forward(self, i, index):
        #embedding
        vi = self.v(i)
        
        w = self.w(index)
        
        hidden_v = self.hidden1(vi)
        hidden_v = self.hidden2(hidden_v)

    
        hidden_w = self.hidden1(w)
        hidden_w = (self.hidden2(hidden_w))


        o = torch.mm(vi, w.T)
        return o
    
    def loss_func(self, output, Mg_p, Mc_p):
        """The loss function in the training
         
        Args:
            output : o is the output generated by forward function
            Mg_p : The transition probability from concept i to concept j in the Mg
            Mc_p : he transition probability from concept i to concept j in the Mc
        Returns:
            _type_: Mean squared loss as shown in Eq 7
        """        
        alpha = 0.
        l_g = 1- self.cos(Mg_p - Mg_p.mean(dim=1, keepdim=True), output - output.mean(dim=1, keepdim=True)) 
        l_c = 1- self.cos(Mc_p - Mc_p.mean(dim=1, keepdim=True), output - output.mean(dim=1, keepdim=True)) 
        loss = alpha * l_c + (1-alpha) * l_g
        return loss.mean()
     

    
    def get_matrix(self):
        #Return the learnt matrix
        o = torch.zeros((self.vocab_size,self.vocab_size))
        res_v = self.v.weight.data
        res_w = self.w.weight.data
        
        res_v = torch.tanh(self.hidden1(res_v))
        res_v = self.hidden2(res_v)
        
        
        res_w = torch.tanh(self.hidden1(res_w))
        res_w = self.hidden2(res_w)

        o = torch.mm(res_v, res_w.T)
        #o = torch.tanh(o)
        return o.data.cpu().numpy()